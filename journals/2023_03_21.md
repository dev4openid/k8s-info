- Fault-tolerance
- On-demand scalability
- Optimal resource usage
- Auto-discovery to automatically discover and communicate with each other
- Accessibility from the outside world
- Seamless updates/rollbacks without any downtime.
-
- Container Orchestrators
- [Amazon Elastic Container Service](https://aws.amazon.com/ecs/)]
	- Amazon Elastic Container Service (ECS) is a hosted service provided by [Amazon Web Services](https://aws.amazon.com/) (AWS) to run containers at scale on its infrastructure.
- [Azure Container Instances](https://azure.microsoft.com/en-us/products/container-instances/#overview)
	- Azure Container Instance (ACI) is a basic container orchestration service provided by [Microsoft Azure](https://azure.microsoft.com/en-us/).
- [Azure Service Fabric](https://azure.microsoft.com/en-us/products/service-fabric/)
  Azure Service Fabric is an open source container orchestrator provided by [Microsoft Azure](https://azure.microsoft.com/en-us/).
- [Kubernetes](https://kubernetes.io/)
	- Kubernetes is an open source orchestration tool, originally started by Google, today part of the [Cloud Native Computing Foundation](https://www.cncf.io/) (CNCF) project.
- [Marathon](https://mesosphere.github.io/marathon/)
	- Marathon is a framework to run containers at scale on [Apache Mesos](https://mesos.apache.org/) and [DC/OS](https://dcos.io/).
- [Nomad](https://www.nomadproject.io/)
	- Nomad is the container and workload orchestrator provided by [HashiCorp](https://www.hashicorp.com/).
	-
- [Docker Swarm](https://docs.docker.com/engine/swarm/)
	- Docker Swarm is a container orchestrator provided by [Docker, Inc](https://www.docker.com/). It is part of [Docker Engine](https://docs.docker.com/engine/).
-
- Look at https://www.edx.org/course/introduction-to-cloud-infrastructure-technologies
-
- **[Kubernetes website](https://kubernetes.io/),**
	- Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications".*
-
- **Borg**, or to lessons learned from it, are:
	- API servers
	- Pods
	- IP-per-Pod
	- Services
	- Labels
- **Kubernetes Features**
	- Automatic bin packing
	- Designed for extensibility
	- Self-healing
	- Horizontal scaling
	- Service discovery and load balancing
- **Additional features**
	- Automated rollouts and rollbacks
	- Secret and configuration management
	- Storage orchestration (mounts software-defined storage (SDS))
	- Batch execution
	- IPv4/IPv6 dual-stack
-
- **Popular graduated projects:**
- [Kubernetes](https://kubernetes.io/) container orchestrator
- [etcd](https://etcd.io/) distributed key-value store
- [CoreDNS](https://coredns.io/) DNS server
- [containerd](http://containerd.io/) container runtime
- [Envoy](https://www.envoyproxy.io/) cloud native proxy
- [Fluentd](http://www.fluentd.org/) for unified logging
- [Harbor](https://goharbor.io/) registry
- [Helm](https://www.helm.sh/) package management for Kubernetes
- [Linkerd](https://linkerd.io/) service mesh for Kubernetes
- [Open Policy Agent](https://www.openpolicyagent.org/) policy engine
- [Prometheus](https://prometheus.io/) monitoring system and time series DB
- [Rook](https://rook.io/) cloud native storage orchestrator for Kubernetes
  
  **Key incubating projects:**
- [argo](https://argoproj.github.io/) workflow engine for Kubernetes
- [Buildpacks.io](https://buildpacks.io/) builds application images
- [CRI-O](https://cri-o.io/) container runtime
- [Contour](https://projectcontour.io/) ingress controller for Kubernetes
- [gRPC](http://www.grpc.io/) for remote procedure call (RPC)
- [CNI](https://www.cni.dev/) for Linux containers networking
- [flux](https://fluxcd.io/) continuous delivery for Kubernetes
- [Knative](https://knative.dev/) serverless containers in Kubernetes
- [KubeVirt](https://kubevirt.io/) Kubernetes based Virtual Machine manager
- [Notary](https://github.com/theupdateframework/notary) for data security
- And many [more](https://www.cncf.io/projects/).
-
- One or more **control plane** nodes
- One or more **worker** nodes (optional, but recommended).
- ![Components of Kubernetes Architecture](https://courses.edx.org/asset-v1:LinuxFoundationX+LFS158x+1T2022+type@asset+block@TrainingImage.png)
- A control plane node runs the following essential control plane components and agents:
	- API Server
	- Scheduler
	- Controller Managers
	- Key-Value Data Store (etcd)
	- In addition, the control plane node runs:
		- Container Runtime
		- Node Agent
		- Proxy
		- Optional add-ons for cluster-level monitoring and logging
# Worker Node Components
- A worker node has the following components:
	- Container Runtime
	- Node Agent - kubelet
	- Proxy - kube-proxy
	- Add-ons for DNS, Dashboard user interface, cluster-level monitoring and logging
# Worker Node Components: Container Runtime
- Kubernetes supports several container runtimes:
- [CRI-O](https://cri-o.io/) A lightweight container runtime for Kubernetes, supporting [quay.io](https://quay.io/) and [Docker Hub](https://hub.docker.com/) image registries.
- [containerd](https://containerd.io/) A simple, robust, and portable container runtime.
- [Docker Engine](https://www.docker.com/)[](https://www.docker.com/) A popular and complex container platform which uses **containerd** as a container runtime.
- [Mirantis Container Runtime](https://www.mirantis.com/software/container-runtime/) Formerly known as the **Docker Enterprise Edition**
- ![Container Runtime Interface](https://courses.edx.org/assets/courseware/v1/9c894d31883fbbb3f920680b46ecf5ed/asset-v1:LinuxFoundationX+LFS158x+1T2022+type@asset+block/Container_Runtime_Interface2023.png)
- **Container Runtime Interface**
- (Retrieved from [blog.kubernetes.io](http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html))
- two services:
	- **ImageService** is responsible for all the image-related operations
	- **RuntimeService** is responsible for all the Pod and container-related operations
- An example of CRI integration to container
- ![cri-containerd](https://courses.edx.org/asset-v1:LinuxFoundationX+LFS158x+1T2022+type@asset+block@cri-containerd2023.png)
- **cri-containerd**
- (Retrieved from [blog.kubernetes.io](http://blog.kubernetes.io/2017/11/containerd-container-runtime-options-kubernetes.html))
# Worker Node Components: Proxy - kube-proxy
- The **kube-proxy** is the network agent which runs on each node
- The kube-proxy is responsible for TCP, UDP, and SCTP stream forwarding or random forwarding across a set of Pod backends of an application, and it implements forwarding rules defined by users through Service API objects
# Worker Node Components: Add-ons
- **DNS** Cluster DNS is a DNS server required to assign DNS records to Kubernetes objects and resources.
- **Dashboard** 
  A general purpose web-based user interface for cluster management.
- **Monitoring** 
  Collects cluster-level container metrics and saves them to a central data store.
- **Logging** 
  Collects cluster-level container logs and saves them to a central log store for analysis
-
- # Networking Challenges
	- networking challenges:
	- Container-to-Container communication inside Pods
	- Pod-to-Pod communication on the same node and across cluster nodes
	- Service-to-Pod communication within the same namespace and across cluster namespaces
	- External-to-Service communication for clients to access applications in a cluster
	  
	  All these networking challenges must be addressed before deploying a Kubernetes cluster
- **Container-to-Container Communication Inside Pods**
	- isolated network space is referred to as a **network namespace**
	- Pod is started, a special infrastructure **Pause container** is initialized by the Container Runtime
- "**IP-per-Pod**" and ensures Pod-to-Pod communication
- **Pod-to-Pod Communication Across Nodes**
	- **localhost** - inside the Pod
	- Containers are integrated with the overall Kubernetes networking model through the use of the [Container Network Interface](https://github.com/containernetworking/cni) (CNI). CNI plugins are 3rd-party Software Defined Networking (SDN)
	- ![Container Network Interface (CNI)](https://courses.edx.org/asset-v1:LinuxFoundationX+LFS158x+1T2022+type@asset+block@Container_Network_Interface__CNI__Core_Plugins2023.png)**Container Network Interface (CNI) ****Core Plugins**
-
- **External-to-Pod Communication**
	- external accessibility through **Services**
	- complex encapsulations of network routing rule definitions stored in **iptables** on cluster nodes and implemented by **kube-proxy** agents
	- with the aid of **kube-proxy**, applications become accessible from outside the cluster over a virtual IP address and a dedicated port number
	-
	-
- # Kubernetes Configuration
- **All-in-One Single-Node Installation**
  In this setup, all the control plane and worker components are installed and running on a single-node. While it is useful for learning, development, and testing, it is not recommended for production purposes.
- **Single-Control Plane and Multi-Worker Installation**
  In this setup, we have a single-control plane node running a stacked etcd instance. Multiple worker nodes can be managed by the control plane node.
- **Single-Control Plane with Single-Node etcd, and Multi-Worker Installation**
  In this setup, we have a single-control plane node with an external etcd instance. Multiple worker nodes can be managed by the control plane node.
- **Multi-Control Plane and Multi-Worker Installation**
  In this setup, we have multiple control plane nodes configured for High-Availability (HA), with each control plane node running a stacked etcd instance. The etcd instances are also configured in an HA etcd cluster and multiple worker nodes can be managed by the HA control plane.
- **Multi-Control Plane with Multi-Node etcd, and Multi-Worker Installation**
  In this setup, we have multiple control plane nodes configured in HA mode, with each control plane node paired with an external etcd instance. The external etcd instances are also configured in an HA etcd cluster, and multiple worker nodes can be managed by the HA control plane. This is the most advanced cluster configuration recommended for production environments
-
# Infrastructure for Kubernetes Installation
- Kubernetes on bare metal, public cloud, private, or hybrid cloud?
- Which underlying OS should we use?
- Which networking solution (CNI) should we use?
-
# Installing Local Learning Clusters
- [Minikube](https://minikube.sigs.k8s.io/docs/) 
  Single- and multi-node local Kubernetes cluster, recommended for a learning environment deployed on a single host.
- [Kind](https://kind.sigs.k8s.io/) 
  Multi-node Kubernetes cluster deployed in Docker containers acting as Kubernetes nodes, recommended for a learning environment.
- [Docker Desktop](https://www.docker.com/products/docker-desktop) 
  Including a local Kubernetes cluster for Docker users.
- [MicroK8s](https://microk8s.io/) 
  Local and cloud Kubernetes cluster for developers and production, from Canonical.
- [K3S](https://k3s.io/) 
  Lightweight Kubernetes cluster for local, cloud, edge, IoT deployments, originally from Rancher, currently a CNCF project.
# Production Clusters from Certified Solutions Providers
# Kubernetes on Windows
- as a [supported](https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/) production ready operating system only for worker nodes of a Kubernetes cluster
-
- # Minikube
- isolation software --driver
- #of cluster nodes, sizes in terms of CPU, memory, and disk space
- Hypervisor can be, in order of preference (on a Linux host): docker, kvm2, podman, vmware, and virtualbox
	- On Linux [VirtualBox](https://www.virtualbox.org/wiki/Downloads), [KVM2](https://www.linux-kvm.org/page/Main_Page), and [QEMU](https://www.qemu.org/) hypervisors, or [Docker](https://docs.docker.com/engine/install/) and [Podman](https://podman.io/getting-started/installation.html) runtimes
	- On macOS [VirtualBox](https://www.virtualbox.org/wiki/Downloads), [HyperKit](https://github.com/moby/hyperkit), [VMware Fusion](http://www.vmware.com/products/fusion.html), [Parallels](https://www.parallels.com/), and [QEMU](https://www.qemu.org/) hypervisors, or [Docker](https://docs.docker.com/desktop/mac/install/) runtime
	- On Windows [VirtualBox](https://www.virtualbox.org/wiki/Downloads), [Hyper-V](https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/quick-start/enable-hyper-v), [VMware Workstation](https://www.vmware.com/in/products/workstation-pro/workstation-pro-evaluation.html), and [QEMU](https://www.qemu.org/) hypervisors, or [Docker](https://docs.docker.com/desktop/windows/install/) runtime
	- **NOTE:** Minikube supports a ***[--driver=none](https://minikube.sigs.k8s.io/docs/drivers/none/)*** (on Linux) option that runs the Kubernetes components bare-metal, directly on the host OS and not inside a VM. With this option a Docker installation is required and a Linux OS on the local workstation, but no hypervisor installation. This driver is recommended for advanced users.
-
- # get creds
- After minikube start
	- (e.g. minikube start -n 3    OR  minikube start --driver=kvm -n 3 --container-runtime=containerd \
	    --cni=calico -p minibox  *Note: the -p name !*)
	- TOKEN=$(kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | rg default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d ':' | tr -d '\t' | tr -d ' ')
	- echo $TOKEN
	- APISERVER=$(kubectl config view | rg  https |  cut -f 2- -d ":" | tr -d ' ')
	- echo $APISERVER
	- curl $APISERVER --header "Authorization: Bearer $TOKEN" --insecure
	- curl $APISERVER/apis/app/v1 --header "Authorization: Bearer $TOKEN" --insecure
- (*there is a problem with script above*)
-
- # Object Model
- With each object, we declare our intent, or the desired state of the object, in the **spec** section
- Kubernetes system manages the **status** section for objects, where it records the actual state of the object and tries to match the desired state vs actual state
- Examples of Kubernetes object types are Nodes, Namespaces, Pods, ReplicaSets, Deployments, DaemonSets, etc.
- **Namespaces**
- The names of the resources/objects created inside a Namespace are unique, but not across Namespaces in the cluster
- Kubernetes creates four Namespaces out of the box: **kube-system**, **kube-public**, **kube-node-lease**, and **default**
- **kube-system** Namespace contains the objects created by the Kubernetes system, mostly the control plane agents
- **default** Namespace contains the objects and resources created by administrators and developers, and objects are assigned to it by default unless another Namespace name is provided by the user
- **kube-public** is a special Namespace, which is unsecured and readable by anyone, used for special purposes such as exposing public (non-sensitive) information about the cluster
- **kube-node-lease** which holds node lease objects used for node heartbeat data
- **new** names are best practice generated to virtualise the cluster and isolate users, developer teams, applications, or tiers - **kubectl create namespace new-namespace-name**
-
- # Labels
- Labels are  **key-value pairs** attached to Kubernetes objects (e.g. Pods, ReplicaSets, Nodes, Namespaces, Persistent Volumes)
- Label selectors
	- **Equality-Based Selectors
	  Equality-Based Selectors allow filtering of objects based on Label keys and values. Matching is achieved using the **=**, **==** (equals, used interchangeably), or **!=** (not equals) operators. For example, with **env==dev** or **env=dev** we are selecting the objects where the **env** Label key is set to value **dev**
	- **Set-Based Selectors
	  Set-Based Selectors allow filtering of objects based on a set of values. We can use **in**, **notin** operators for Label values, and **exist/does not exist** operators for Label keys. For example, with **env in (dev,qa)** we are selecting objects where the **env** Label is set to either **dev** or **qa**; with **!app** we select objects with no Label key **app**.
	- # ReplicationControllers
	- Do not use by itself (set actual pod replicas to desired state)
	- Prefer use of Deployment which configures a ReplicaSet controller to manage the lifecycle requirement
	-